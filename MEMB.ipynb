{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d4b91b-9f73-4097-82bd-76dcc2f8f63c",
   "metadata": {},
   "source": [
    "<h1> Maximum Excluded Mass Burning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f70bb3-15ee-4f4b-b41b-20077863e914",
   "metadata": {},
   "source": [
    "The Maximum Excluded Mass Burning (MEMB) algorithm [1] calculates the box-covering for a given network as follows:\n",
    "\n",
    "1. Initially, mark all nodes as uncovered and non-centres.\n",
    "2. For each non-centre node calculate the excluded mass. The excluded mass is defined as the number of uncovered nodes within a radius $r_B$ of the node.\n",
    "3. Let the node with the maximum excluded mass be $p$, let $p$ be a centre and let all nodes within a radius $r_B$ from $p$ be covered.\n",
    "4. Repeat steps 2 and 3 until all nodes in the network are covered. \n",
    "\n",
    "Once a list of centres has been found as above, the boxes in the network are found as follows:\n",
    "\n",
    "1. Assign each centre node a box ID.\n",
    "2. For each node $v$ calculate the central distance, which is the minimum $d(u,v)$ where $u$ is a centre. All centres have a a central distance of 0 and all nodes have central distance strictly less than $r_B$.\n",
    "3. Rearrange all non-centre nodes in order of increasing central distance.\n",
    "4. Each non-centre node must have at least one neighbour with central distance less than their own. Assign each node to the box of this neighbour. If there are multiple options for this neighbour, then choose randomly. Repeat this step for all non-centres in the order found by step 3.\n",
    "\n",
    "The code in this notebook calculates the box-covering according to the original MEMB algorithm as well as an amended accelerated method based on the degree centrality of nodes. It also finds the renormalised graphs under $\\ell_B$ box renormalisation and can identify if a network has fractal properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd2713-db24-4563-884b-4ea9e827e75c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Module Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "343b7c8c-09cc-49a6-a132-0a0045647636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from operator import itemgetter\n",
    "from scipy.io import mmread\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8cc1e-b3e5-4129-b28f-518b02ccecd5",
   "metadata": {},
   "source": [
    "<h2> Tutorial <a class=\"anchor\" id=\"workspace\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f83ad9-5013-45e7-a9e3-71d9aa439702",
   "metadata": {},
   "source": [
    "This area of the notebook can be used to work with the functions given, and some examples are provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98371b77-5c10-4cdc-b53f-cf8389d4ab26",
   "metadata": {},
   "source": [
    "**Reading Graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a2792-e719-469b-93c6-dd7b88482221",
   "metadata": {},
   "source": [
    "Graphs can be read in from multiple file formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fbfe586e-33d4-42f9-bbd2-3cca78bd807e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read file in .mtx format\n",
    "eduG = read_mtx_graph_format(\"web-edu.mtx\")\n",
    "# Read file in .gml format\n",
    "uvflowerG = nx.read_gml(\"32flower4iter.gml\")\n",
    "# Read edge list file in .txt format\n",
    "notredameG = nx.read_edgelist(\"web-NotreDame.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2a636-8ea3-4cd8-9680-a900a69f83ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use `summarise_graph` to display an output of the network's key features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "175d815d-5ab2-432c-8d7f-0bc01115dcc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network has 3031 nodes and 6474 edges.\n",
      "The average degree of the network is 4.27185747278126.\n",
      "The average shortest path length is 4.273095287093869.\n",
      "The diameter is 11.\n"
     ]
    }
   ],
   "source": [
    "summarise_graph(eduG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bd119-7d74-4908-93da-f9ff45b791a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Finding Centres using MEMB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb3477-fc89-4d47-90f4-10c56f858892",
   "metadata": {},
   "source": [
    "The original MEMB method is applied using `MEMB` with a given network and value $\\ell_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "4e626c5f-b145-463d-8077-4826b3a7a0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '23', '26', '29', '32', '35', '38', '41', '44', '47', '50', '53', '56', '59', '62', '65', '68', '71', '74', '77', '80', '83', '86', '89', '92', '21', '22', '24', '25', '27', '28', '30', '31', '33', '34', '36', '37', '39', '40', '42', '43', '45', '46', '48', '49', '51', '52', '54', '55', '57', '58', '60', '61', '63', '64', '66', '67', '69', '70', '72', '73', '75', '76', '78', '79', '81', '82', '84', '85', '87', '88', '90', '91', '93', '94', '95', '98', '101', '104', '107', '110', '113', '116', '119', '122', '125', '128', '131', '134', '137', '140', '143', '146', '149', '152', '155', '158', '161', '164', '167', '170', '173', '176', '179', '182', '185', '188', '191', '194', '197', '200', '203', '206', '209', '212', '215', '218', '221', '224', '227', '230', '233', '236', '239', '242', '245', '248', '251', '254', '257', '260', '263', '266', '269', '272', '275', '278', '281', '284', '287', '290', '293', '296', '299', '302', '305', '308', '311', '314', '317', '320', '323', '326', '329', '332', '335', '338', '341', '344', '347', '350', '353', '356', '359', '362', '365', '368', '371', '374', '377', '380', '383', '386', '389', '392', '395', '398', '401', '404', '407', '410', '413', '416', '419', '422', '425', '428', '431', '434', '437', '440', '443', '446', '449', '452', '455', '458', '461', '464', '467']\n"
     ]
    }
   ],
   "source": [
    "centres = MEMB(uvflowerG, 5)\n",
    "print(centres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f71b1-0fa6-446e-ad25-349762e4abed",
   "metadata": {},
   "source": [
    "The amended method, accelerated using the degrees of nodes, is applied using the function `degree_based_MEMB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ea465-f992-419e-96fd-2515ff716cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centres = degree_based_MEMB(notredameG, 17)\n",
    "print(centres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5e76b-59ed-4f54-b25f-66fa7785f75e",
   "metadata": {},
   "source": [
    "The methods can be compared using the function `compare_MEMB_methods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78449b9-a3d6-48b6-b25b-af7bab465d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_MEMB_methods(eduG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908c9c6-c156-4e84-8b3c-962b8b6e022b",
   "metadata": {},
   "source": [
    "To find the values of $N_B$ for all $\\ell_B = 1, \\dots, \\Delta$ where $\\Delta$ is the diameter of the network use *what*\n",
    "\n",
    "Note that the MEMB methods only work for odd values of $\\ell_B$, so these are the only values checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9618a-1023-4c48-a2bf-fc5d12de9b38",
   "metadata": {},
   "source": [
    "**Finding the Best Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071f984-3b26-4c6c-904d-f3c0aff092f4",
   "metadata": {},
   "source": [
    "The functions `find_best_fractal_fit` and `find_best_exp_fit` find the power-law fit of the form $N_B = A\\ell_B^{-c}$ and exponential fit of the form $N_B = Ae^{-c\\ell_B}$ which best fit the given data, and give their sum of squares regression score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94e1a2-8d36-47aa-af44-5405dc1cd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "find_best_fractal_fit("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2759f9-f53b-41c1-8b8b-eb9181143587",
   "metadata": {},
   "source": [
    "<h2> Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81b4493a-4bf3-4352-847a-bdfdc607def7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lB=1 the amended method finds the same number of centres, 2732.\n",
      "It does so in 0.0% of the time: 0.0 seconds compared to 97.88867950439453 seconds.\n",
      "For lB=3 the amended method finds the same number of centres, 684.\n",
      "It does so in 18.25287735418049% of the time: 26.06658673286438 seconds compared to 142.80809664726257 seconds.\n",
      "For lB=5 the amended method finds the same number of centres, 172.\n",
      "It does so in 17.39591002369012% of the time: 5.727313995361328 seconds compared to 32.92333650588989 seconds.\n",
      "For lB=7 the amended method finds the same number of centres, 172.\n",
      "It does so in 16.494901383624498% of the time: 7.681981801986694 seconds compared to 46.57185649871826 seconds.\n",
      "For lB=9 the amended method finds the same number of centres, 44.\n",
      "It does so in 18.389379771872182% of the time: 1.8421025276184082 seconds compared to 10.017208576202393 seconds.\n",
      "For lB=11 the amended method finds the same number of centres, 44.\n",
      "It does so in 23.32438521019799% of the time: 3.364976644515991 seconds compared to 14.426861047744751 seconds.\n",
      "For lB=13 the amended method finds the same number of centres, 44.\n",
      "It does so in 16.089778029535644% of the time: 2.772505521774292 seconds compared to 17.2314715385437 seconds.\n",
      "For lB=15 the amended method finds the same number of centres, 44.\n",
      "It does so in 17.399777272033276% of the time: 3.0961246490478516 seconds compared to 17.794047594070435 seconds.\n",
      "For lB=17 the amended method finds the same number of centres, 12.\n",
      "It does so in 21.588605787191174% of the time: 1.853963851928711 seconds compared to 8.587696075439453 seconds.\n",
      "For lB=19 the amended method finds the same number of centres, 12.\n",
      "It does so in 20.861553592403258% of the time: 2.127460479736328 seconds compared to 10.197996377944946 seconds.\n",
      "For lB=21 the amended method finds the same number of centres, 12.\n",
      "It does so in 20.27422316034528% of the time: 2.3919408321380615 seconds compared to 11.797940731048584 seconds.\n",
      "For lB=23 the amended method finds the same number of centres, 12.\n",
      "It does so in 21.323865409331766% of the time: 2.8585500717163086 seconds compared to 13.405402898788452 seconds.\n",
      "For lB=25 the amended method finds the same number of centres, 12.\n",
      "It does so in 18.1290525485824% of the time: 2.8168225288391113 seconds compared to 15.537615776062012 seconds.\n",
      "For lB=27 the amended method finds the same number of centres, 12.\n",
      "It does so in 19.820840658069592% of the time: 3.3060357570648193 seconds compared to 16.679594039916992 seconds.\n",
      "For lB=29 the amended method finds the same number of centres, 12.\n",
      "It does so in 17.720743543080893% of the time: 3.5032031536102295 seconds compared to 19.768939971923828 seconds.\n",
      "For lB=31 the amended method finds the same number of centres, 12.\n",
      "It does so in 18.813205858329262% of the time: 4.084296941757202 seconds compared to 21.709733963012695 seconds.\n",
      "For lB=33 the amended method finds the same number of centres, 4.\n",
      "It does so in 19.307282448773645% of the time: 3.8484373092651367 seconds compared to 19.932568550109863 seconds.\n",
      "For lB=35 the amended method finds the same number of centres, 4.\n",
      "It does so in 19.395560822116902% of the time: 4.2103869915008545 seconds compared to 21.707993030548096 seconds.\n",
      "For lB=37 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.353725352256493% of the time: 4.306875228881836 seconds compared to 23.465945720672607 seconds.\n",
      "For lB=39 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.68233173279518% of the time: 4.764472484588623 seconds compared to 25.50255799293518 seconds.\n",
      "For lB=41 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.144781146167492% of the time: 5.043190002441406 seconds compared to 27.79416275024414 seconds.\n",
      "For lB=43 the amended method finds the same number of centres, 4.\n",
      "It does so in 19.606681658368295% of the time: 5.645777702331543 seconds compared to 28.795171976089478 seconds.\n",
      "For lB=45 the amended method finds the same number of centres, 4.\n",
      "It does so in 17.30503943455954% of the time: 5.512233734130859 seconds compared to 31.853343963623047 seconds.\n",
      "For lB=47 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.34141423797606% of the time: 5.8167760372161865 seconds compared to 31.713890552520752 seconds.\n",
      "For lB=49 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.55564146830553% of the time: 6.20081901550293 seconds compared to 33.41743278503418 seconds.\n",
      "For lB=51 the amended method finds the same number of centres, 4.\n",
      "It does so in 17.906983868612592% of the time: 7.00433087348938 seconds compared to 39.115078926086426 seconds.\n",
      "For lB=53 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.619716879591074% of the time: 7.194728851318359 seconds compared to 38.64037728309631 seconds.\n",
      "For lB=55 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.95963438148851% of the time: 8.004395246505737 seconds compared to 42.21808862686157 seconds.\n",
      "For lB=57 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.750138925872783% of the time: 8.672019004821777 seconds compared to 46.25042533874512 seconds.\n",
      "For lB=59 the amended method finds the same number of centres, 4.\n",
      "It does so in 19.820265496457612% of the time: 9.678267240524292 seconds compared to 48.830159425735474 seconds.\n",
      "For lB=61 the amended method finds the same number of centres, 4.\n",
      "It does so in 18.617856858040017% of the time: 10.131846904754639 seconds compared to 54.42004942893982 seconds.\n",
      "For lB=63 the amended method finds the same number of centres, 4.\n",
      "It does so in 17.706273981870048% of the time: 9.821527242660522 seconds compared to 55.46919274330139 seconds.\n",
      "For lB=65 the amended method finds the same number of centres, 2.\n",
      "It does so in 17.847418001116893% of the time: 10.619279623031616 seconds compared to 59.50036931037903 seconds.\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_gml(\"22flower5iter.gml\")\n",
    "compare_MEMB_methods(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e317787-bdbe-4b73-bc67-53025f6f3859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Is this renormalisation?\n",
    "current_graph = G.copy()\n",
    "lB = 3\n",
    "count = 1\n",
    "while len(list(current_graph.nodes())) > 1:\n",
    "    new_graph = main(current_graph, lB, count)\n",
    "    current_graph = new_graph.copy()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87eb1e4-ca91-4c85-aae2-5d046e63304f",
   "metadata": {},
   "source": [
    "<h1> Results <a class=\"anchor\" id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14604cfd-84ba-4cfe-a2f8-4392d03c9b8c",
   "metadata": {},
   "source": [
    "This section stores the results for the optimal number of boxes $N_B$ with diameter $\\ell_B = 1, \\dots, \\Delta$ where $\\Delta$ is the diameter of the network $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ff2d2-4e0b-44f2-bec9-344d9e3209a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19731a-0940-4a25-b81f-e84c12b89a08",
   "metadata": {},
   "source": [
    "<h3>$(u,v)$-flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859d43a-57c7-481b-8611-cfdbb716aa6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2,3)-flower with 3 Iterations\n",
    "lB = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "NB = [470, 95, 45, 20, 20, 10, 10, 6, 5, 5, 5, 4, 3, 2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db400e17-b718-41dd-82d0-4198cda3ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1,3)-flower with 4 Iterations\n",
    "lB = np.array([1, 3, 5, 7, 9, 11, 13, 15])\n",
    "NB = np.array([684, 172, 44, 12, 4, 2, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea283147-62ed-4e64-aa17-4a47c5ee5fbe",
   "metadata": {},
   "source": [
    "<h2> Real-World Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb273cf3-db8d-4aa3-aa24-16c3e21535af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> World Wide Web Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa1ebb-ff0c-405d-94e0-01a935e83f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# web-edu.mtx Network\n",
    "lB = [1, 3, 5, 7, 9, 11, 13]\n",
    "NB = [3031, 249, 12, 6, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce89a4-74e4-443b-afd1-d78ac527d4c1",
   "metadata": {},
   "source": [
    "<h1> Functions <a class=\"anchor\" id=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9c6c8-74f2-4558-ad11-5e01d5c29cb3",
   "metadata": {},
   "source": [
    "<h2> 1 Reading Graphs <a class=\"anchor\" id=\"read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4d854-8ef2-4957-816f-ee92c44815a7",
   "metadata": {},
   "source": [
    "<h3> 1.1 Reading File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbc127-7c8c-4b6b-a99e-877138aae7a9",
   "metadata": {},
   "source": [
    "Many graphs are stored in the .mtx file format and need to be converted to a format readable by networkX. Some may need to be edited to make sure that they are in a readable format. Check that the header starts with %%MatrixMarket (with two % signs) and that the second line is a list of three numbers. The function `read_mtx_graph_format` reads graphs stored in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfbfd97-63fc-4719-be34-73a18a90d4c5",
   "metadata": {},
   "source": [
    "**Read Graphs in .mtx File Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0fb9bb85-3933-4b0d-b0c9-a9d2a27bc880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_mtx_graph_format(filepath):\n",
    "    \"\"\"\n",
    "    Reads graphs stored in the .mtx file format. Use with, for example, graphs from www.networkrepository.com.\n",
    "    Note: Some files may need to be edited to make sure that scipy.io can read them. Files should have a header starting with %%MatrixMarket and a single line denoted the number of values in each column. \n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Filepath to .mtx file\n",
    "        \n",
    "    Returns:\n",
    "        G (networkx.Graph): Network read from file. \n",
    "    \"\"\"\n",
    "    # Read the file using the scipy.io file reader. \n",
    "    mmf = mmread(filepath)\n",
    "    # Generate a graph from this file. \n",
    "    G = nx.from_scipy_sparse_array(mmf)\n",
    "    # Return the graph.\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f8ce9-bdac-4d69-bade-d5d67a9b01bb",
   "metadata": {},
   "source": [
    "<h3> 1.2 Finding Graph Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d547d46-f6cb-4606-8fb2-80d735ddb456",
   "metadata": {},
   "source": [
    "The following function `summarise_graph` summarises the key properties of the network given. Calculating the diameter and the average shortest path length is expensive, and so this is only recommended for graphs with relatively few nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "77a6119c-51f5-45ec-828a-b1b590a7a739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarise_graph(G, skip_diam=False, skip_aspl=False):\n",
    "    \"\"\"\n",
    "    Summarises the key attributes of a given network. \n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed.\n",
    "        skip_diam (Bool) (opt): If True, then do not calculate the diameter of the graph. This is recommended for large graphs. The default is False.\n",
    "        skip_aspl (Bool) (opt): If True, then do not calculate the average shortest path length of the graph. This is recommended for large graphs. The default is False. \n",
    "    \n",
    "    Returns:\n",
    "        None    \n",
    "    \"\"\"\n",
    "    # Display the size and order of the network.\n",
    "    print(\"Network has {0} nodes and {1} edges.\".format(len(G.nodes()), len(G.edges())))\n",
    "    \n",
    "    # Calculate and display the average degree of the network.\n",
    "    degree_dist = [deg for (node, deg) in G.degree()]\n",
    "    avg_degree = sum(degree_dist)/len(degree_dist)\n",
    "    print(\"The average degree of the network is {0}.\".format(avg_degree))\n",
    "    \n",
    "    # If chosen, find the average shortest path length.\n",
    "    if not skip_aspl:\n",
    "        print(\"The average shortest path length is {0}.\".format(nx.average_shortest_path_length(G)))\n",
    "        \n",
    "    # If chosen, find the diameter.\n",
    "    if not skip_diam:\n",
    "        print(\"The diameter is {0}.\".format(nx.diameter(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10480cf5-1b1c-4cf3-867d-50dfd21e3444",
   "metadata": {},
   "source": [
    "<h2> 2 Maximal Excluded Mass Burning Algorithms <a class=\"anchor\" id=\"MEMB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c25ea-fcc8-4c14-95f5-12a0c28c02ec",
   "metadata": {},
   "source": [
    "<h3>2.1 MEMB Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1b1e5-e2e6-4e48-bdae-d1be431aca90",
   "metadata": {},
   "source": [
    "The following functions implement the Maximal Excluded Mass Burning algorithm: first the original method in `MEMB` and second the amended method based on degree centrality in `degree_based_MEMB` (see [2] for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56809f-4019-483f-9541-f3609bc2360e",
   "metadata": {},
   "source": [
    "**Original MEMB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ebf11f5-edcd-4f95-ae67-14d02949c970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MEMB(G, lB, deterministic=True):\n",
    "    \"\"\"\n",
    "    Implements the Maximal Excluded Mass Burning algorithm as according to [1].\n",
    "    Note: Only works for odd numbers of lB, otherwise it takes rB to be the floor of (lB-1)/2 which is the same as for the odd lB-1.\n",
    "    \n",
    "    Args: \n",
    "        G (networkx.Graph): The network the algorithm is to be applied to. \n",
    "        lB (int): The diameter of the boxes used to cover the network.\n",
    "        deterministic (Bool) (opt): If False, choose fom nodes with equal excluded mass randomly. If True, choose the first lexicographically.\n",
    "    \n",
    "    Returns: \n",
    "        centres (list): A list of nodes assigned to be centres under the MEMB algorithm. \n",
    "    \"\"\"\n",
    "    # Start with all nodes being uncovered and non-centres. \n",
    "    uncovered = list(G.nodes())\n",
    "    non_centres = list(G.nodes())\n",
    "    \n",
    "    # Initialise empty lists for the covered and centre nodes. \n",
    "    covered = []\n",
    "    centres = []\n",
    "    \n",
    "    # Each box can have diameter of up to lB, so the maximum radius is rB = (lB-1)/2.\n",
    "    rB = (lB-1)/2\n",
    "    \n",
    "    # Initialise an empty dictionary to store nodes and a list of nodes in the graphs centred on these nodes with a radius rB.\n",
    "    # Doing this stops us from having to generate the same subgraphs multiple times, which is expensive. \n",
    "    eg_dict = {}\n",
    "    \n",
    "    # For each node find the graph centres on that node with radius rB. \n",
    "    for node in G.nodes():\n",
    "        H = nx.ego_graph(G, node, radius=rB)\n",
    "        eg_dict[node] = list(H.nodes()) # Add the list of nodes in that graph to the dictionary.\n",
    "\n",
    "    # Iterate while there are still nodes uncovered in the graph.\n",
    "    while len(uncovered) > 0:\n",
    "\n",
    "        # Start with a maximum excluded mass of zero, and no node p [1].\n",
    "        p = None\n",
    "        maximum_excluded_mass = 0\n",
    "        \n",
    "        # For the non-deterministic method, keep a list of nodes with equal maximum excluded mass \n",
    "        possible_p = []\n",
    "\n",
    "        # For each node that isn't a centre, find the excluded mass.\n",
    "        for node in non_centres:\n",
    "            # The excluded mass is the number of uncovered nodes in within a radius of rB.\n",
    "            excluded_mass = len(list(set(eg_dict[node])-set(covered)))\n",
    "            # If the excluded mass of this node is greater than the current excluded mass, choose this node.\n",
    "            if excluded_mass > maximum_excluded_mass:\n",
    "                p = node # Update p.\n",
    "                maximum_excluded_mass = excluded_mass # Update maximum excluded mass.\n",
    "                possible_p = [node] # Update list of possible nodes for non-deterministic method.\n",
    "            # If the excluded mass of this node is equal to the current maximum excluded mass, then add this node to the list of possible p.\n",
    "            elif excluded_mass == maximum_excluded_mass:\n",
    "                possible_p.append(node)\n",
    "        \n",
    "        # If the non-deterministic method is chosen, then randomly choose a node from the list of possible p.\n",
    "        if not deterministic:\n",
    "            p = random.choice(possible_p)\n",
    "                \n",
    "        # Add the chosen p to the list of centres. \n",
    "        centres.append(p)\n",
    "        # Remove the chosen p from the list of non-centres.\n",
    "        non_centres.remove(p)\n",
    "\n",
    "        # Find the graph centred on the node p with radius rB.\n",
    "        H = eg_dict[p] \n",
    "        # Iterate through the nodes in this subgraph.\n",
    "        for node in H:\n",
    "            covered.append(node) # Cover the nodes in the subgraph.\n",
    "            # Remove these nodes from the list of uncovered nodes.\n",
    "            if node in uncovered:\n",
    "                uncovered.remove(node)        \n",
    "    \n",
    "    # Once all the nodes are covered, return the list of centres. \n",
    "    return centres\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19911d5b-c990-4d3d-87c3-8a96e7ab13c9",
   "metadata": {},
   "source": [
    "**Degree Based MEMB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "329ff81a-e901-4272-8b40-7e46602d936e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def degree_based_MEMB(G, lB, deterministic=True, N=10):\n",
    "    \"\"\"\n",
    "    Implements the Maximal Excluded Mass Burning algorithm as according to [1], adjusted to prioritise nodes with high degree. \n",
    "    This reduces the running time of the traditional MEMB without losing accuracy.\n",
    "    Note: Only works for odd numbers of lB, otherwise it takes rB to be the floor of (lB-1)/2 which is the same as for the odd lB-1.\n",
    "    \n",
    "    Args: \n",
    "        G (networkx.Graph): The network the algorithm is to be applied to. \n",
    "        lB (int): The diameter of the boxes used to cover the network.\n",
    "        deterministic (Bool) (opt): If False, choose fom nodes with equal excluded mass randomly. If True, choose the first lexicographically.\n",
    "        N (int): Takes the top N-th nodes by degree to find the centred subgraph of. \n",
    "    \n",
    "    Returns: \n",
    "        centres (list): A list of nodes assigned to be centres under the MEMB algorithm. \n",
    "    \"\"\"\n",
    "    \n",
    "    # If the diameter lB is less than or equal to 2, then the maximum radius is 0 and so every node is in its own box.\n",
    "    if lB == 1 or lB == 2:\n",
    "        return list(G.nodes())\n",
    "    \n",
    "    # Start with all nodes being uncovered and non-centres. \n",
    "    uncovered = list(G.nodes())\n",
    "    \n",
    "    # Initialise empty lists for the covered and centre nodes. \n",
    "    covered = []\n",
    "    centres = []\n",
    "    \n",
    "    # Each box can have diameter of up to lB, so the maximum radius is rB = (lB-1)/2.\n",
    "    rB = (lB-1)/2\n",
    "\n",
    "    # Find the N nodes with the greatest degree centrality. \n",
    "    dc = nx.degree_centrality(G)\n",
    "    top_N_dc = dict(sorted(dc.items(), key=itemgetter(1), reverse=True)[:N])\n",
    "\n",
    "    # Initialise an empty dictionary to store nodes and a list of nodes in the graphs centred on these nodes with a radius rB.\n",
    "    # Doing this stops us from having to generate the same subgraphs multiple times, which is expensive. \n",
    "    eg_dict = {}\n",
    "    for node in top_N_dc:\n",
    "        H = nx.ego_graph(G, node, radius=rB)\n",
    "        eg_dict[node] = list(H.nodes())\n",
    "    \n",
    "    # On the initial iteration, set maiden to True.\n",
    "    maiden = True\n",
    "    \n",
    "    # This variable checks if the algorithm does not find a solution, and then looks at the next N nodes. \n",
    "    failed = False\n",
    "    \n",
    "    # Iterate while there are still nodes uncovered in the graph.\n",
    "    while len(uncovered) > 0:\n",
    "        \n",
    "        # For all iterations except the first, calculate the new dictionary of nodes in each subgraph of radius rB.\n",
    "        if not maiden:\n",
    "            eg_dict = calc_next_iter(G, uncovered, N, dc, eg_dict, rB, centres, p, failed)\n",
    "        maiden = False\n",
    "\n",
    "        # Start with a maximum excluded mass of zero, and no node p [1].\n",
    "        p = None\n",
    "        maximum_excluded_mass = 0\n",
    "        \n",
    "        # For the non-deterministic method, keep a list of nodes with equal maximum excluded mass \n",
    "        possible_p = []\n",
    "        \n",
    "        # For each of the top N nodes by degree, find the excluded mass. \n",
    "        for node in eg_dict:\n",
    "            # The excluded mass is the number of uncovered nodes in within a radius of rB. \n",
    "            excluded_mass = len(list(set(eg_dict[node])-set(covered)))\n",
    "            # If the excluded mass of this node is greater than the current excluded mass, choose this node.\n",
    "            if excluded_mass > maximum_excluded_mass:\n",
    "                p = node # Update p.\n",
    "                maximum_excluded_mass = excluded_mass # Update maximum excluded mass. \n",
    "                possible_p = [node] # Update list of possible nodes for non-deterministic method.\n",
    "            # If the excluded mass of this node is equal to the current maximum excluded mass, then add this node to the list of possible p.\n",
    "            elif excluded_mass == maximum_excluded_mass:\n",
    "                possible_p.append(node)\n",
    "            \n",
    "        # If the non-deterministic method is chosen, then randomly choose a node from the list of possible p.\n",
    "        if not deterministic:\n",
    "            p = random.choice(possible_p)\n",
    "        \n",
    "        # Check if the method fails to find a node p. \n",
    "        # The method only fails if every single node has zero uncovered nodes within a radius rB. \n",
    "        if p == None:\n",
    "            # If it does fail, remove all the nodes that have already been tried from the list of top N nodes. \n",
    "            failed=True # Set the failed variable.\n",
    "            for tried_node in eg_dict:\n",
    "                dc.pop(tried_node)\n",
    "        else:    \n",
    "            # Otherwise, add the new p to the list of centre nodes. \n",
    "            centres.append(p)\n",
    "            # Update the list so that the newly covered nodes are in the list of covered nodes and not in the list of uncovered nodes. \n",
    "            for node in eg_dict[p]:\n",
    "                if node in uncovered:\n",
    "                    uncovered.remove(node)\n",
    "                    covered.append(node)\n",
    "            failed=False # Reset the failed variable.\n",
    "\n",
    "    # Once all the nodes are covered return the list of centres found in the graph.\n",
    "    return centres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dfa51-c16f-4597-bf93-e02c8ae66f1d",
   "metadata": {},
   "source": [
    "**Calculating the Next Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc382d07-6cb0-40ba-8a6e-0704d51542e0",
   "metadata": {},
   "source": [
    "`calc_next_iter` updates the values to be checked in the next iteration of the amended degree based MEMB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8c8c50d9-1520-4ac1-b951-5ec29a6cc114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_next_iter(G, uncovered,  N, dc, eg_dict, rB, centres, p, failed):\n",
    "    \"\"\"\n",
    "    Finds the top N nodes to check in the next iteration of the algorithm.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network being analysed. \n",
    "        uncovered (list): A list of nodes in the network which are uncovered at this stage.\n",
    "        N (int): The number of nodes to check in the next iteration of the algorithm.\n",
    "        dc (dict): Dictionary containing the degree centrality of each node which is yet to be checked as a centre.\n",
    "        eg_dict (dict): Dictionary with keys as nodes and values as the list of nodes within a distance of rB from that node.\n",
    "        rB (int): The radius rB to be checked. \n",
    "        centres (list): List of nodes identified as centres. \n",
    "        p (str): Name of the node chosen as the most recent p [1].\n",
    "        failed (Bool): True if the previous iteration of the algorithm failed, and False otherwise. \n",
    "    \n",
    "    Returns:\n",
    "        eg_dict (dict): Returns an updated version of eg_dict with the nodes to be checked for the next iteration. \n",
    "    \"\"\"\n",
    "    \n",
    "    # If the algorithm failed in the previous iteration, then remove the most recent node p from the dictionary. \n",
    "    # This node was chosen as a centre in the last stage, and so it does not need to be considered again.\n",
    "    if not failed:\n",
    "        dc.pop(p)\n",
    "        \n",
    "    # Choose the top N nodes by degree centrality. \n",
    "    top_N_dc = dict(sorted(dc.items(), key=itemgetter(1), reverse=True)[:N])\n",
    "    \n",
    "    # Initialise an empty updated version of eg_dict.\n",
    "    new_eg_dict = {}\n",
    "\n",
    "    # For each of the top N nodes, assign the list of nodes in the subgraph of radius rB centred around the node to the new dictionary.\n",
    "    for node in top_N_dc:\n",
    "        # If the subgraph has already been found then reference the old dictionary to prevent recalculating the subgraph.\n",
    "        if node in eg_dict:\n",
    "            new_eg_dict[node] = eg_dict[node]\n",
    "        # If not, then find the graph and add it to the new dictionary.\n",
    "        else:\n",
    "            H = nx.ego_graph(G, node, radius=rB)\n",
    "            new_eg_dict[node] = list(H.nodes())\n",
    "    \n",
    "    # Once this is done for all the relevant nodes, return the new updated dictionary.\n",
    "    return new_eg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7785471-296e-4cc6-ab93-e4bbffeb4c51",
   "metadata": {},
   "source": [
    "<h3> 2.2 Calculating the Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493d712-a127-43ca-8738-20bbee531285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO DO \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0262e-0fe9-4735-9bfc-fc71b0e68e8c",
   "metadata": {},
   "source": [
    "<h3> 2.3 Comparing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb1f60-5095-4d27-af45-662585129643",
   "metadata": {},
   "source": [
    "The functions in the following section are used to compare the perfomance of the two algorithms. The function `original_method` runs and times the original method, and the function `degree_method` runs and times the amended method. Running the final function `compare_MEMB_methods` will apply both algorithms to the given network and compare the results. It can also take different methods as an argument and compare those to the original MEMB method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee1f51-8663-4f46-8587-0d2469b5f9fd",
   "metadata": {},
   "source": [
    "**Original Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "16e089e1-3357-4b24-992c-78aede16747f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def original_method(G, lB):\n",
    "    \"\"\" \n",
    "    Runs and times the original MEMB algorithm.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): Network to be analysed. \n",
    "        lB (int): Diameter for box covering. \n",
    "        \n",
    "    Returns: \n",
    "        centres (list): A list of centres found by the MEMB algorithm.\n",
    "        time_elapsed (float): The time in seconds it took for the algorithm to complete. \n",
    "    \"\"\"\n",
    "    start = time.time() # Start timer\n",
    "    centres = MEMB(G,lB) # Find centres by algorithm\n",
    "    end = time.time() # End timer\n",
    "    time_elapsed = end-start # Find time elapsed\n",
    "    return centres, time_elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bd652-e8b8-4a11-8a10-ba4d94e06b05",
   "metadata": {},
   "source": [
    "**Degree Based Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f908a9a6-dd56-4473-8697-3e0d77599436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def degree_method(G, lB):\n",
    "    \"\"\" \n",
    "    Runs and times the amended degree based MEMB algorithm.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): Network to be analysed. \n",
    "        lB (int): Diameter for box covering. \n",
    "        \n",
    "    Returns: \n",
    "        centres (list): A list of centres found by the amended algorithm.\n",
    "        time_elapsed (float): The time in seconds it took for the algorithm to complete. \n",
    "    \"\"\"\n",
    "    start = time.time() # Start timer\n",
    "    centres = degree_based_MEMB(G,lB,N=500) # Find centres by algorithm\n",
    "    end = time.time() # End timer\n",
    "    time_elapsed = end-start # Find time elapsed\n",
    "    return centres, time_elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53f438-99cd-4b7b-b70f-57aac6c36791",
   "metadata": {},
   "source": [
    "**Compare Different Versions of the MEMB Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "40503ee2-7817-4bd8-aaa8-f13206530a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_MEMB_methods(G, diam=None, amended_method=degree_method):\n",
    "    \"\"\"\n",
    "    Compares the performance of the original MEMB algorithm with an amended algorithm - the default being the degree based method. \n",
    "    The algorithms are run for all values of lB up to the diameter of the network and the number of centres found and the time taken is compared. \n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed. \n",
    "        diam (int) (opt): The diameter of the network G. The default is None, and note that if no diameter is given the algorithm will calculate it which is expensive for large networks.\n",
    "        amended_method (func): The method to be compared against the original MEMB. Default is the degree based amended MEMB algorithm (see [2]).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # If no diameter is given for the network then it is calculated using networkX. \n",
    "    if diam == None:\n",
    "        diam = nx.diameter(G)\n",
    "        \n",
    "    # The MEMB algorithm only works for odd numbers (see MEMB function docstrings or [2] for explanation).\n",
    "    # Therefore, find the next biggest odd number. \n",
    "    nearest_odd = int(np.ceil(diam) // 2 * 2 + 1)\n",
    "    \n",
    "    # Iterate for each value of lB from 1 to the diameter (or one plus the diameter, if the diameter is even).\n",
    "    for lB in range(1, nearest_odd + 2, 2):\n",
    "        original_centres, original_time = original_method(G, lB) # Find the centres and time taken for the original method.\n",
    "        amended_centres, amended_time = amended_method(G, lB) # Find the centres and time taken for the amended method.\n",
    "        \n",
    "        # Find the number of centres found by each method.\n",
    "        no_original_centres = len(original_centres)\n",
    "        no_amended_centres = len(amended_centres)\n",
    "        \n",
    "        # If both methods find the same number of centres, then print out a statement saying so. \n",
    "        if no_original_centres == no_amended_centres:\n",
    "            print(\"For lB={0} the amended method finds the same number of centres, {1}.\".format(lB, no_original_centres))\n",
    "        # If the amended method finds a different number of centres (assumed to be more) then print out a statement with that information.\n",
    "        # We can assume that the number of centres will be more because the MEMB algorithm checks every possible node for a centre, so every amendment should not be able to do better. \n",
    "        else:\n",
    "            print(\"For lB={0} the amended method finds {1} more centres: {2} compared to {3}.\".format(lB, no_amended_centres-no_original_centres, no_amended_centres, no_original_centres))\n",
    "        # Print the difference in time taken for the two methods. \n",
    "        print(\"It does so in {0:.2f}% of the time: {1} seconds compared to {2} seconds.\".format((amended_time/original_time)*100, amended_time, original_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f5c85-7d09-4b05-bc7b-1de31622424c",
   "metadata": {},
   "source": [
    "<h2>3 Finding the Best Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a3b22-587a-4ea5-86db-f1d084868e1f",
   "metadata": {},
   "source": [
    "It is known that in fractal networks the relationship between the diameter of the box $\\ell_B$ and the number of boxes $N_B$ is given by,\n",
    "\\begin{equation}\n",
    " N_B \\sim \\ell_B ^{-d_B},\n",
    "\\end{equation}\n",
    "whereas for non-fractal networks we see a relationship of the form,\n",
    "\\begin{equation}\n",
    " N_B \\sim e ^ {-\\ell_Bd_B}.\n",
    "\\end{equation}\n",
    "To determine if a network is fractal or not we need to determine which of the two distributions better fits the network. To do this, we use the sum of squares regression to see how similar one distribution is to another. We can find the best exponential (non-fractal) fit and the best power-law (fractal) fit by varying the constants $A$ and $c$ in the following two relations:\n",
    "\\begin{align}\n",
    " N_B = Ae ^ {-c\\ell_B}, && \\text{and} && N_B = A\\ell_B ^{-c}.\n",
    "\\end{align}\n",
    "Then we can find the \"best fit of the best fits\" to determine which relationship the network follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0193f0-c5fd-4422-8a31-b3a02fec662a",
   "metadata": {},
   "source": [
    "<h3>3.1 Sum of Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b5a04-2703-4496-b5ea-c5e722c6f89e",
   "metadata": {},
   "source": [
    "The sum of squares regression value is defined as,\n",
    "\\begin{equation}\n",
    " \\text{SSR} := \\sum_{\\ell_B} (N_B(\\ell_B) - N'(\\ell_B))^2,\n",
    "\\end{equation}\n",
    "where $N_B$ is the empirical optimal number of boxes in the network and $N'$ is the number according to some model. \n",
    "If the two distributions are very similar, then each term in the sum will be close to zero and $\\text{SSR}$ will be small. If they are very different then $\\text{SSR}$ will be very large. Thus to find a best fit we want to minimise the value of $\\text{SSR}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0bb24a02-878a-4e42-8719-9e72bc4f2cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sum_of_squares_deviation(y, est_y):\n",
    "    \"\"\" \n",
    "    Finds the value SSR for the sum of squares regression method using a true and model distribution. \n",
    "    \n",
    "    Args:\n",
    "        y (list): The true or measured distribution.\n",
    "        est_y (list): The model distribution to be compared. \n",
    "    \n",
    "    Returns:\n",
    "        sum_of_squares (float): The sum of squares regression\n",
    "    \"\"\"\n",
    "    sum_of_squares = 0 # Initialise the sum as zero.\n",
    "    # Iterate for each pair of values in the true/model distributions. \n",
    "    for (yi, est_yi) in zip(y, est_y):\n",
    "        # Add to the sum the square of the difference between the two distributions. \n",
    "        sum_of_squares += (est_yi - yi) ** 2\n",
    "    # Return the total sum of the squares. \n",
    "    return sum_of_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee156c-11fa-4d7e-a7ef-32300b9e7a27",
   "metadata": {},
   "source": [
    "<h3>3.2 Finding the Best Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000b50f-6ada-4740-a33f-397572a96055",
   "metadata": {},
   "source": [
    "**Finding the Best Exponential Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea927a9-d1f5-4518-af82-6de38550f601",
   "metadata": {},
   "source": [
    "The function `find_best_exp_fit` finds the best approximation of the form $N_B = Ae ^ {-c\\ell_B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "df0545b6-27fa-43ae-a995-31f6f1955dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_exp_fit(x, y, A_min=0, A_max=100, c_min=0, c_max=2, linspace_N=100):\n",
    "    \"\"\"\n",
    "    Finds the best exponential fit of the form y = Ae^{-cx} according to the sum of squares deviation.\n",
    "    \n",
    "    Args: \n",
    "        x (list): The values of x in the distribution.\n",
    "        y (list): The values of y in the distribution.\n",
    "        A_min (int) (opt): The minimum value of A to be tested. Can be adjusted to find more accurate results. Default is 0.\n",
    "        A_max (int) (opt): The maximum value of A to be tested. Can be adjusted to find more accurate results. Default is 100.\n",
    "        c_min (int) (opt): The minimum value of c to be tested. Can be adjusted to find more accurate results. Default is 0. \n",
    "        c_max (int) (opt): The maximum value of c to be tested. Can be adjusted to find more accurate results. Default is 2. \n",
    "        linspace_N (int) (opt): The number of values of A and c to be checked in the respective ranges. Default is 100.\n",
    "        \n",
    "    Returns:\n",
    "        best_fit (tuple): The coefficients A and c from the best exponential approximation. \n",
    "        best_score (float): The sum of squares regression of this approximation.\n",
    "    \"\"\"\n",
    "    # Initialise empty variables for the best fit (i.e. best A and c) and the best SSR score. \n",
    "    best_fit = (None, None)\n",
    "    best_score = None\n",
    "    \n",
    "    # Iterate through linspace_N values of A in the range [A_min, A_max].\n",
    "    for A in np.linspace(A_min, A_max, linspace_N):\n",
    "        # Iterate through linspace_N values of c in the range [c_min, c_max].\n",
    "        for c in np.linspace(c_min, c_max, linspace_N):\n",
    "            \n",
    "            # Find the values of y according to the exponential model with parameters A and c.\n",
    "            est_y = [A * math.e ** (-c*i) for i in x]\n",
    "            # Calculate the sum of squares regression.\n",
    "            score = sum_of_squares_deviation(y, est_y)\n",
    "            \n",
    "            # If the best score is yet to be updated (i.e. this is the first iteration) then set the current A, c and SSR to the best fit values.\n",
    "            if best_score == None:\n",
    "                best_score = score\n",
    "                best_fit = (A, c)\n",
    "            # If the new SSR score is smaller than the current best, then update the best score and update the best fit to the current A and c.\n",
    "            elif score < best_score:\n",
    "                best_score = score\n",
    "                best_fit = (A, c)\n",
    "                \n",
    "    # Once all values are tried return the best fit and the best score.\n",
    "    return best_fit, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd352f0-8cff-4bba-bea1-74cc9b9cda6d",
   "metadata": {},
   "source": [
    "**Finding the Best Power Law Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee48981-23d2-42bf-8c96-5e0d2296d356",
   "metadata": {},
   "source": [
    "The function `find_best_fractal_fit` finds the best approximation of the form $N_B = A\\ell_B ^{-c}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0c2145af-d7f9-4e18-b4a4-cef40447cd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_fractal_fit(x, y, A_min=0, A_max=10000, c_min = 0, c_max = 12.5, linspace_N=100):\n",
    "    \"\"\"\n",
    "    Finds the best exponential fit according to the sum of squares deviation of the form y = Ax^{-c}\n",
    "    \n",
    "    Args: \n",
    "        x (list): The values of x in the distribution.\n",
    "        y (list): The values of y in the distribution.\n",
    "        A_min (int) (opt): The minimum value of A to be tested. Can be adjusted to find more accurate results. Default is 0.\n",
    "        A_max (int) (opt): The maximum value of A to be tested. Can be adjusted to find more accurate results. Default is 10000.\n",
    "        c_min (int) (opt): The minimum value of c to be tested. Can be adjusted to find more accurate results. Default is 0. \n",
    "        c_max (int) (opt): The maximum value of c to be tested. Can be adjusted to find more accurate results. Default is 12.5. \n",
    "        linspace_N (int) (opt): The number of values of A and c to be checked in the respective ranges. Default is 100.\n",
    "        \n",
    "    Returns:\n",
    "        best_fit (tuple): The coefficients A and c from the best power law approximation. \n",
    "        best_score (float): The sum of squares regression of this approximation.\n",
    "    \"\"\"\n",
    "    # Initialise empty variables for the best fit (i.e. best A and c) and the best SSR score. \n",
    "    best_fit = (None, None)\n",
    "    best_score = None\n",
    "    \n",
    "    # Iterate through linspace_N values of A in the range [A_min, A_max].\n",
    "    for A in np.linspace(A_min, A_max, 100):\n",
    "        # Iterate through linspace_N values of c in the range [c_min, c_max].\n",
    "        for c in np.linspace(c_min, c_max, 100):\n",
    "            \n",
    "            # Find the values of y according to the power law fractal model with parameters A and c.\n",
    "            est_y = [A * i ** (-c) for i in x]\n",
    "            # Calculate the sum of squares regression.\n",
    "            score = sum_of_squares_deviation(y, est_y)\n",
    "            \n",
    "            # If the best score is yet to be updated (i.e. this is the first iteration) then set the current A, c and SSR to the best fit values.\n",
    "            if best_score == None:\n",
    "                best_score = score\n",
    "                best_fit = (A, c)\n",
    "            # If the new SSR score is smaller than the current best, then update the best score and update the best fit to the current A and c.\n",
    "            elif score < best_score:\n",
    "                best_score = score\n",
    "                best_fit = (A, c)\n",
    "    \n",
    "    # Once all values are tried return the best fit and the best score.\n",
    "    return best_fit, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbda0c-bde7-4ed0-8a28-d9a0ea3d4f06",
   "metadata": {},
   "source": [
    "**Finding the Best Fit Recursively**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0e23e-7907-40de-8c09-744dd75d6045",
   "metadata": {},
   "source": [
    "`find_best_fit_iteratively` iteratively reduces the range $[A_\\min, A_\\max]$ and the range $[c_\\min, c_\\max]$ to find a more accurate best fit values of $A$ and $c$ by updating the values of `A_min`, `A_max`, `c_min` and `c_max` in each iteration. At each stage a new range $\\frac 1{10}$th of the size of the original is found for $A$, and $1{5}$th of the size of the original is found for $c$. By default the method uses the original ranges $[0,10000]$ and $[0,12.5]$ and 4 iterations so that the final round uses an interval with range $10$ for $A$ and an interval with range $0.1$ for $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b1adc0f2-3107-463c-a458-d48d9df24463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_best_fit_iteratively(x, y, method, linspace_N=100, A_min=0, A_max=10000, c_min=0, c_max=12.5, iter_num=4):\n",
    "    \"\"\"\n",
    "    Finds the best fit according to a given model by iteratively reducing the range of values checked for A and c.\n",
    "    \n",
    "    Args:\n",
    "        x (list): The values of x in the distribution.\n",
    "        y (list): The values of y in the distribution.\n",
    "        method (func): A function which finds the best fit to the given distribution according to a given model.\n",
    "        linspace_N (int) (opt): The number of values of A and c to be checked in the respective ranges. Default is 100.\n",
    "        A_min (int) (opt): The minimum value of A to be tested. Can be adjusted to find more accurate results. Default is 0.\n",
    "        A_max (int) (opt): The maximum value of A to be tested. Can be adjusted to find more accurate results. Default is 10000.\n",
    "        c_min (int) (opt): The minimum value of c to be tested. Can be adjusted to find more accurate results. Default is 0. \n",
    "        c_max (int) (opt): The maximum value of c to be tested. Can be adjusted to find more accurate results. Default is 12.5. \n",
    "        iter_num (int) (opt): The number of iterations to complete. \n",
    "        \n",
    "    Returns:\n",
    "        best_fit (tuple): The coefficients A and c from the best approximation. \n",
    "        best_score (float): The sum of squares regression of this approximation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the current range of the intervals for A and c.\n",
    "    A_diff = A_max - A_min\n",
    "    c_diff = c_max - c_min\n",
    "    \n",
    "    # Iterate as many times as dictated by the iter_num variable.\n",
    "    for i in range(iter_num):\n",
    "        \n",
    "        print(A_min, A_max)\n",
    "        print(c_min, c_max)\n",
    "        \n",
    "        # Find the best fit in this range according to the intervals given.\n",
    "        best_fit, best_score = method(x, y, A_min=A_min, A_max=A_max, c_min=c_min, c_max=c_max, linspace_N=linspace_N)\n",
    "        \n",
    "        # Reduce the interval range for A by a factor of 10 and for c by a factor of 5.\n",
    "        A_diff = A_diff / 10\n",
    "        c_diff = c_diff / 5\n",
    "        \n",
    "        # Extract the values of A and c from the current best fit.\n",
    "        best_A_approximation = best_fit[0]\n",
    "        best_c_approximation = best_fit[1]\n",
    "        \n",
    "        # Adjust the interval [A_min, A_max] to have a range of A_diff about the current best estimate for A.\n",
    "        A_min = max(0, best_A_approximation - (A_diff/2))\n",
    "        A_max = best_A_approximation + (A_diff/2)\n",
    "        \n",
    "        # Adjust the interval [c_min, c_max] to have a range of c_diff about the current best estimate for c.\n",
    "        c_min = max(0, best_c_approximation - (c_diff/2))\n",
    "        c_max = best_c_approximation + (c_diff/2)\n",
    "\n",
    "    # Once iter_num iterations are complete, return the best fit and best score.\n",
    "    return best_fit, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483c810-26b2-4810-adb7-0a1c7437d352",
   "metadata": {},
   "source": [
    "<h2> 4 Plotting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80143e4c-24e3-4fc3-a7f8-4dbf350343bf",
   "metadata": {},
   "source": [
    "<h3> 4.1 Plotting $N_B$ against $\\ell_B$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dbe59f-82c3-49a7-8ecb-be3c759d2d57",
   "metadata": {},
   "source": [
    "Plotting $N_B$ against $\\ell_B$ should reveal if a network is fractal or not. A fractal network will follow a power law relation and a non-fractal network will follow an exponential relation (see Section 3). Plotting on a log log scale will reveal a straight line for fractal networks and a curved line for non-fractal networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82744b49-cd3c-4184-b6e1-54b23c4abb6a",
   "metadata": {},
   "source": [
    "**Plot $N_B$ against $\\ell_B$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff17b5-c69a-4a42-b174-365fffeb160c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lB_NB(lB, NB):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the optimal number of boxes NB against the diameter of the boxes lB.\n",
    "    \n",
    "    Args:\n",
    "        lB (list): List of the values of lB.\n",
    "        NB (list): List of the corresponding values of NB.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Plots distribution using matplotlib.\n",
    "    plt.plot(lB, NB, color='#C00000')\n",
    "    plt.xlabel('$\\ell_B$')\n",
    "    plt.ylabel('$N_B$')\n",
    "    plt.title('The optimal number of boxes $N_B$ against the diameter $\\ell_B$.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310860a0-fee5-4a08-a93e-ac2373f2462f",
   "metadata": {},
   "source": [
    "**Plot $N_B$ against $\\ell_B$ on Log Log Scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d2010-b500-4c76-a43c-2a5c84aee67a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loglog_lB_NB(lB, NB):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the optimal number of boxes NB against the diameter of the boxes lB on a log log scale.\n",
    "    \n",
    "    Args:\n",
    "        lB (list): List of the values of lB.\n",
    "        NB (list): List of the corresponding values of NB.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Plots distribution on log log scale using matplotlib.\n",
    "    plt.loglog(lB, NB, color='#C00000')\n",
    "    plt.xlabel('$\\ell_B$')\n",
    "    plt.ylabel('$N_B$')\n",
    "    plt.title('The optimal number of boxes $N_B$ against the diameter $\\ell_B$.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209e5ab-6c8e-4a1d-be03-6a947583cedf",
   "metadata": {},
   "source": [
    "<h3>4.2 Comparing the Different Fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6abc9-c311-4841-af37-eb07922d9ea4",
   "metadata": {},
   "source": [
    "The function `plot_best_fit_comparison` plots the best fit from the fractal model against the best fit from the non-fractal model to see which is a better fit for the given distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1771673-e14a-44ca-8823-0a24deea7674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_best_fit_comparison(lB, NB, exp_A, exp_c, exp_score, frac_A, frac_c, frac_score):\n",
    "    \"\"\"\n",
    "    Plots a comparison of the best power law (fractal) and exponential (non-fractal) fit for a given distribution. \n",
    "    \n",
    "    Args:\n",
    "        lB (list): List of the values of lB.\n",
    "        NB (list): List of the corresponding values of NB.\n",
    "        exp_A (float): The optimal value of A according to the exponential fit. \n",
    "        exp_c (float): The optimal value of c according to the exponential fit. \n",
    "        exp_score (float): The sum of squares regression for the exponential best fit.\n",
    "        frac_A (float): The optimal value of A according to the power law fit. \n",
    "        frac_c (float): The optimal value of c according to the power law fit.\n",
    "        frac_score (float): The sum of squares regression for the power law best fit.\n",
    "    \"\"\"\n",
    "    # The lists lB and NB need to be converted to numpy arrays.\n",
    "    lB = np.array(lB)\n",
    "    NB = np.array(NB)\n",
    "    \n",
    "    est_NB_exp = [exp_A * math.e ** (-exp_c*i) for i in lB] # Find the exponential fit according to A and c given.\n",
    "    est_NB_frac = [frac_A * i ** (-frac_c) for i in lB] # Find the power law fit according to A and c given.\n",
    "    \n",
    "    # Initialise a plot.\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "    \n",
    "    # Plot the distribution against the power law fit.\n",
    "    # Note we only use one set of labels for the legend as both plots use the same colours. \n",
    "    axes[0].plot(lB, NB, color='#000066', label=\"Empirical Data\")\n",
    "    axes[0].plot(lB, est_NB_frac, color='#C00000', label=\"Best Fit\")\n",
    "    \n",
    "    # Plot the distribution against the exponential fit.\n",
    "    axes[1].plot(lB, NB, color='#000066')\n",
    "    axes[1].plot(lB, est_NB_exp, color='#C00000')\n",
    "\n",
    "    fig.suptitle('Non-Fractal Network Model', fontsize=16) # Title the plot.\n",
    "\n",
    "    # Label the axes and title the subplot.\n",
    "    axes[0].set_xlabel('$\\ell_B$')\n",
    "    axes[0].set_title('Power-Law Relation')\n",
    "    axes[0].set_ylabel('$N_B$')\n",
    "    axes[0].text(12, 600, r\"$SSR \\approx ${0}\".format(frac_score)) # Write the SSR score on the plot.\n",
    "\n",
    "    # Label the axes and title the subplot.\n",
    "    axes[1].set_xlabel('$\\ell_B$')\n",
    "    axes[1].set_title('Exponential Relation')\n",
    "    axes[1].set_ylabel('$N_B$')\n",
    "    axes[1].text(12, 600, r\"$SSR \\approx ${0}\".format(exp_score)) # Write the SSR score on the plot.\n",
    "\n",
    "    fig.legend(loc=\"upper left\") # Add the legend.\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.plot() # Display the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db18a5-9ac1-4050-81ac-6470c6790d85",
   "metadata": {},
   "source": [
    "<h2> 5 Finding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4d3d2-1186-4e45-8f0e-98bc897475c2",
   "metadata": {},
   "source": [
    "In this section the second stage of the MEMB algorithm is applied: assigning nodes to boxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c197216-543a-4079-a8b3-aa1dbc2f4a9a",
   "metadata": {},
   "source": [
    "<h3>5.1 Calculating Central Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd71ab9-de57-4480-b638-3b27953fb109",
   "metadata": {},
   "source": [
    "The central distance is defined as the minimum distance from a node to any of the centre nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91604519-cbc2-40f6-b9fe-ae79ab459417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_central_distance(G, centres):\n",
    "    \"\"\"\n",
    "    Finds the central distance for each node in a network given a list of centres. \n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed. \n",
    "        centres (list): A list of centre nodes from the MEMB algorithm. \n",
    "        \n",
    "    Returns:\n",
    "        central_distance (dict): A dictionary containing nodes as keys and their central distance as values. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise an empty dictionary to store the values for the central distance. \n",
    "    central_distance = {}\n",
    "    \n",
    "    # Iterate through each of the nodes in the network.\n",
    "    for v in list(G.nodes()):\n",
    "        \n",
    "        # Initialise an empty variable for the shortest path length.\n",
    "        shortest_path_len = None\n",
    "        \n",
    "        # If the node v is a centre then it must have central distance 0, so check for this case to speed up the algorithm.\n",
    "        if v in centres:\n",
    "            central_distance[v] = 0\n",
    "            \n",
    "        # For all non-centre nodes v, iterate through the list of all centres. \n",
    "        else:\n",
    "            for u in centres:\n",
    "                # Find the shortest path length between the node v and a centre u.\n",
    "                path_len = nx.shortest_path_length(G, v, u)\n",
    "                \n",
    "                # If this is a new minimum, then update the shortest path variable\n",
    "                if shortest_path_len == None or shortest_path_len > path_len:\n",
    "                    shortest_path_len = path_len\n",
    "            \n",
    "            # Assign the value of the shortest path length to the node in the dictionary.\n",
    "            central_distance[v] = shortest_path_len\n",
    "            \n",
    "    # Once all nodes are checked return the values in the dictionary.\n",
    "    return central_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a46736-0101-4962-8dce-5ddd04d696f3",
   "metadata": {},
   "source": [
    "<h3> 5.2 Assigning Nodes to Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1694a7-3d32-4431-aece-d1fe054ba99c",
   "metadata": {},
   "source": [
    "The process for finding boxes happens in two stages. First, each node is given a box. Then using that information, a list of nodes is generated for each box. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135deac1-4961-4990-9ee6-52d627c257fc",
   "metadata": {},
   "source": [
    "**Giving Each Node a Box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f32fd7-5cb0-4adb-841f-dbe7be1bac70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_nodes_to_boxes(G, centres, central_distance):\n",
    "    \"\"\"\n",
    "    Generates a dictionary assigning each node to a box under the MEMB algorithm. \n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed. \n",
    "        centres (list): A list of centre nodes according to the MEMB algorithm.\n",
    "        central_distance (dict): A dictionary containing nodes as keys and their central distance as values.\n",
    "        \n",
    "    Returns:\n",
    "        nodes_to_boxes (dict): A dictionary containing nodes as keys and the box they are assigned to as the value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise an empty dictionary to store the boxes for each node. \n",
    "    nodes_to_boxes = {}\n",
    "    \n",
    "    # Find the list of all nodes which are non-centres. \n",
    "    nodes = list(G.nodes())\n",
    "    non_centres = list(set(nodes) - set(centres))\n",
    "    \n",
    "    # The following section of code produces a list of non-centres in order of increasing central distance.\n",
    "    sorted_non_centres = [] # Initialise an empty list of non-centres.\n",
    "    sorted_dict = dict(sorted(central_distance.items(), key=itemgetter(1))) # Sort the dictionary of central distances into increasing order.\n",
    "    # Add each node to the list of sorted non-centres in order. \n",
    "    for key in sorted_dict:\n",
    "        if not key in centres:\n",
    "            sorted_non_centres.append(key)\n",
    "\n",
    "    id = 0 # The ID of the first box is zero.\n",
    "    # For each of the centres assign a unique box ID.\n",
    "    for node in centres:\n",
    "        nodes_to_boxes[node] = id\n",
    "        id += 1 # Increment the box ID.\n",
    "        \n",
    "    # Iterate through each of the non-centres \n",
    "    for node in sorted_non_centres:\n",
    "        # Initialise an empty list of possible boxes the node can belong to.\n",
    "        possible_boxes = []\n",
    "        # Find the neighbours which have central distance strictly less than the current node.\n",
    "        for neighbour in G.neighbors(node):\n",
    "            if central_distance[node] > central_distance[neighbour]:\n",
    "                # For each, add their box to the list of possible boxes for the current node.\n",
    "                possible_boxes.append(boxes[neighbour])\n",
    "        # Make a random choice from the list of possible boxes and assign that box to the node.\n",
    "        nodes_to_boxes[node] = random.choice(possible_boxes)\n",
    "        \n",
    "    # Once all nodes have been checked return a dictionary containing the mapping from all of the nodes to a box. \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8975fe-0a95-402e-91a1-f8b7f4dd1b46",
   "metadata": {},
   "source": [
    "**Giving Each Box a List of Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f98045-331f-44a7-919a-881c52bd7de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_boxes(nodes_to_boxes, centres):\n",
    "    \"\"\"\n",
    "    Finds a list of nodes assigned to each box in a network.\n",
    "    \n",
    "    Args:\n",
    "        nodes_to_boxes (dict): A dictionary with nodes as keys and their corresponding boxes as values. \n",
    "        centres (list): A list of the nodes found as centres under the MEMB algorithm. \n",
    "    \n",
    "    Returns:\n",
    "        boxes (dict): A dictionary with boxes as keys and a list of nodes in that box as the value. \n",
    "    \"\"\"\n",
    "    # Initialise an empty dictionary to store the boxes.\n",
    "    boxes = {}\n",
    "    \n",
    "    # The box IDs are 0, ..., k-1 where k is the number of centres. \n",
    "    for i in range(len(centres)): # Iterate over the box IDs.\n",
    "        # Initialise an empty list of nodes.\n",
    "        nodes = []\n",
    "        # Check if each node belongs in the current box.\n",
    "        for node in nodes_to_boxes:\n",
    "            # If it does, add it to the list of nodes.\n",
    "            if nodes_to_boxes[node] == i:\n",
    "                nodes.append(node)\n",
    "                \n",
    "        # Assign the list of nodes to the box. \n",
    "        boxes[i] = nodes\n",
    "        \n",
    "    # Return the dictionary of boxes to nodes. \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f2f51-8875-48c0-8f26-2df9d84eb38a",
   "metadata": {},
   "source": [
    "<h2> 6. Box-Renormalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b90340-a2be-4ac5-a8f4-a7c0747cc376",
   "metadata": {},
   "source": [
    "Once boxes have been found according to the MEMB algorithm, the network can be renormalised by replacing each box with a supernode and creating an edge between two supernodes if an edge existed between any pair of nodes in each of their original respective boxes. This is equivalent to \"zooming out\" on the graph and is used to examine self-similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d645c-abf2-410c-bcbd-188f54b7c71f",
   "metadata": {},
   "source": [
    "<h3> 6.1 Renormalising Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098d2ab-53d8-4002-824a-37e6942aff82",
   "metadata": {},
   "source": [
    "`renormalise_graph` renormalises a given network according to the box-covering procedure described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1feeb4-b6a6-475a-9f0b-bfc803a38aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def renormalise_graph(G, boxes, nodes_to_boxes, draw=False):\n",
    "    \"\"\"\n",
    "    Renormalise a graph under a given box-covering.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed. \n",
    "        boxes (dict): A dictionary with boxes as keys and a list of nodes in that box as the value. \n",
    "        nodes_to_boxes (dict): A dictionary with nodes as keys and their corresponding boxes as values. \n",
    "        draw (Bool) (opt): If True then displays the renormalised graph. Default is False. \n",
    "        \n",
    "    Returns: \n",
    "        renormalisedG (networkx.Graph): The network under renormalisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise an empty graph to be the renormalised graph of G. \n",
    "    renormalisedG = nx.Graph()\n",
    "    \n",
    "    # Add one supernode for each of the boxes found under the MEMB algorithm.\n",
    "    for box in boxes:\n",
    "        renormalisedG.add_node(box)\n",
    "        \n",
    "    # Iterate through each of the edges in the original graph.\n",
    "    for edge in G.edges():\n",
    "        \n",
    "        # Find the nodes originally connected by the edge.\n",
    "        source = edge[0]\n",
    "        target = edge[1]\n",
    "        \n",
    "        # Find the supernodes these nodes now belong to.\n",
    "        renormalised_source = nodes_to_boxes[source]\n",
    "        renormalised_target = nodes_to_boxes[target]\n",
    "        \n",
    "        # Create a new edge between the supernodes. \n",
    "        renormalisedG.add_edge(renormalised_source, renormalised_target)\n",
    "        \n",
    "    # Simplify the graph by removing any self loops (edges from a supernode to itself).\n",
    "    renormalisedG.remove_edges_from(nx.selfloop_edges(renormalisedG))\n",
    "    \n",
    "    # If draw is True then display the network using the Kamada Kawai layout.\n",
    "    if draw:\n",
    "        nx.draw_kamada_kawai(renormalisedG, node_color = list(renormalisedG.nodes()))\n",
    "        \n",
    "    # Return the renormalised graph.\n",
    "    return renormalisedG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b255a2c-df6d-40c1-9e00-e259b15398b7",
   "metadata": {},
   "source": [
    "<h3> 6.2 Visualising Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e288b65-c589-4e3c-b35f-0fff3f92abdb",
   "metadata": {},
   "source": [
    "There are many network visualisation tools more powerful than networkX or matplotlib. The following functions reformat graphs so that they can be read by these softwares and so that you can see how a box is renormalised as a supernode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fe1d5-03cd-4f82-8cf1-a1e8daf0fa31",
   "metadata": {},
   "source": [
    "**Gephi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908337e8-2ad8-41be-a079-8b13a3b10d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_to_gephi(G, nodes_to_boxes, file_path):\n",
    "    \"\"\"\n",
    "    Puts graphs in a format readable to Gephi including attributes for the boxes found under box coverings. \n",
    "    \n",
    "    Args:\n",
    "        G (networkx.Graph): The network to be analysed. \n",
    "        nodes_to_boxes (dict): A dictionary with nodes as keys and their corresponding boxes as values. \n",
    "        file_path (str): The path for the gml file to be saved to. \n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    H = G.copy() # Create a copy of the network.\n",
    "    \n",
    "    # Assign to each node an attribute according to its box given under the box covering.\n",
    "    nx.set_node_attributes(H, nodes_to_boxes, 'boxes')\n",
    "    \n",
    "    # Write the graph including the box covering attributes to the file path.\n",
    "    nx.write_gml(H, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ad2c8-5915-45b4-9318-8ef5671586b4",
   "metadata": {},
   "source": [
    "<h1> TO SORT OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceeed1d-8d94-4f15-8f90-f308d276934f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(G, lB, count=1):\n",
    "\n",
    "    centres = degree_based_MEMB(G, lB, deterministic=True, N=10)\n",
    "    central_distance = find_central_distance(G, centres)\n",
    "    nodes_to_boxes = assign_nodes_to_boxes(G, centres, central_distance)\n",
    "    colourmap = []\n",
    "    for node in G.nodes():\n",
    "        colourmap.append(nodes_to_boxes[node])\n",
    "    plt.figure(1)\n",
    "    nx.draw_kamada_kawai(G, node_color = colourmap)\n",
    "    boxes = find_boxes(nodes_to_boxes, centres)\n",
    "    \n",
    "    print(\"Iteration {0}\".format(count))\n",
    "    box_sizes = []\n",
    "    for box in boxes:\n",
    "        box_sizes.append(len(boxes[box]))\n",
    "    print(len(G.nodes()), box_sizes)\n",
    "    \n",
    "    boxes_file_path = \"32flower5iter_boxes_\" + str(count)\n",
    "    renormalised_file_path = \"32flower5iter_renormalised_\" + str(count)\n",
    "    \n",
    "    export_to_gephi(G, nodes_to_boxes, boxes_file_path)\n",
    "    plt.figure(2)\n",
    "    renormalisedG = renormalise_graph(G, boxes, nodes_to_boxes)\n",
    "    gephi_dict = {}\n",
    "    for node in renormalisedG:\n",
    "        gephi_dict[node] = node\n",
    "    \n",
    "    export_to_gephi(renormalisedG, gephi_dict, renormalised_file_path)\n",
    "    plt.show()\n",
    "    return renormalisedG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b4ac3-5238-4a92-b691-3ea151e53e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#H = nx.watts_strogatz_graph(100, 4,0.7)\n",
    "eduG = read_mtx_graph_format(\"web-edu.mtx\")\n",
    "current_graph = eduG.copy()\n",
    "lB = 3\n",
    "count = 1\n",
    "while len(list(current_graph.nodes())) > 1:\n",
    "    new_graph = main(current_graph, lB, count)\n",
    "    current_graph = new_graph.copy()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb471e-6b4e-4325-b02a-bfb706b47a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "H = nx.watts_strogatz_graph(25, 4,0.7)\n",
    "centres = degree_based_MEMB(H, 3, deterministic=True, N=10)\n",
    "central_distance = find_central_distance(H, centres)\n",
    "nodes_to_boxes = assign_nodes_to_boxes(H, centres, central_distance)\n",
    "colourmap = []\n",
    "for node in H.nodes():\n",
    "    colourmap.append(nodes_to_boxes[node])\n",
    "nx.draw_kamada_kawai(H, with_labels=True, node_color = colourmap)\n",
    "boxes = find_boxes(nodes_to_boxes, centres)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606db420-4f13-417a-9afa-b99f56d5693f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "renormalise_graph(H, boxes, nodes_to_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8eff5-5c11-4fba-aac7-792611e0d2f0",
   "metadata": {},
   "source": [
    "<h1> References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5da5b-7617-4956-8089-d22a58a15e2c",
   "metadata": {},
   "source": [
    "[1] C. Song, L. K. Gallos, S. Havlin, and H. A. Makse, “How to calculate the fractal dimension of a complex\n",
    "network: The box covering algorithm,” Journal of Statistical Mechanics, 2007.\n",
    "\n",
    "[2] This will be a reference to my thesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
